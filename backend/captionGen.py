import torch
from PIL import Image

from transformers import (
    BlipProcessor,
    BlipForConditionalGeneration,
    AutoModelForCausalLM,
    AutoTokenizer,
)

# Check if CUDA (GPU support) is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.cuda.empty_cache()  # empty vram


class captionGen:
    """
    This is the model wrapper for the BLIP Image Captioning Model
    and Zephyr 7B LLM Model
    """

    def __init__(self):
        """
        Create wrapper for BLIP Model
        """
        self.processor = BlipProcessor.from_pretrained(
            "Salesforce/blip-image-captioning-large"
        )
        self.model = BlipForConditionalGeneration.from_pretrained(
            "Salesforce/blip-image-captioning-large"
        ).to(device)

    def blip_create_caption(self, imageurl):
        """
        BLIP Model prediction
        :input
            - image: SVG, PNG, JPG, or GIF file
        :returns a caption based off the image file
        """
        raw_image = Image.open(imageurl).convert("RGB")
        text = "a photo of"
        inputs = self.processor(raw_image, text, return_tensors="pt").to(
            device
        )

        out = self.model.generate(**inputs)
        return self.processor.decode(out[0], skip_special_tokens=True)

    def llm_create_caption(self, caption, tone):
        """
        Zephyr 7B Model prediction
        :input
            - caption: generated by BLIP (blip_create_caption)
            - tone: preset string appended to caption choosen by user
        :returns a caption based off the BLIP caption and tone
        """
        messages = [
            {
                "role": "system",
                "content": "You are a bot that generates one "
                + tone
                + " Instagram caption",
            },
            {"role": "user", "content": caption},
        ]
        # Create wrapper for Zephyr LLM Model
        tokenizer = AutoTokenizer.from_pretrained(
            "HuggingFaceH4/zephyr-7b-beta"
        )
        model = AutoModelForCausalLM.from_pretrained(
            "HuggingFaceH4/zephyr-7b-beta", torch_dtype=torch.float16
        ).to(device)
        tokenized_chat = tokenizer.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_tensors="pt",
        ).to(device)

        # Zephyr LLM predict
        output = model.generate(
            tokenized_chat,
            max_length=140,  # caption length
            num_return_sequences=1,  # Generate multiple sequences
            temperature=0.7,  # Adjust the temperature
            top_k=50,  # Adjust the top_k parameter
            top_p=0.9,  # Adjust the top_p parameter
            repetition_penalty=1.3,  # Penalize repetition
            do_sample=True,  # Sample from the distribution
        )

        generated_joke = tokenizer.decode(output[0], skip_special_tokens=True)
        torch.cuda.empty_cache()  # empty vram
        return generated_joke.partition("<|assistant|>")[2] # extract caption from LLM chat output


if __name__ == "__main__":
    location = "uploads/wildcamping.jpg"
    gen = captionGen()
    caption = gen.blip_create_caption(location)
    print("Predicted caption: " + caption)
    funnycaption = gen.llm_create_caption(caption, "Funny")
    print("Funny caption: " + funnycaption)
